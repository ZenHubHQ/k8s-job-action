# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# !!!!!!!!!!!!!! This repository is public !!!!!!!!!!!!!!!!!!!!!!!!
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# repo is public until Github Action supports cloning private repos
# https://github.com/github/roadmap/issues/74
name: 'Run Kubernetes Job'
description: 'Create a K8s Job and tails its logs until it fails or complete'
inputs:
  namespace:  # id of input
    description: 'namespace to use (can be also created and/or deleted after use)'
    required: true
  name:
    description: 'job name to create in namespace'
    required: true
  image:
    description: 'image to run the job on'
    required: true
  command:
    description: command the job will run, can be "cmd arg" or "['cmd', 'arg']"
    required: false
    default: "[]"
  createNamespace:
    description: '[true/false] whether to create the namespace, default to false'
    required: false
    default: 'false'
  deleteNamespace:
    description: '[true/false] whether to delete the namespace if the job complete in time'
    required: false
    default: 'false'
  timeoutMinuteStartContainer:
    description: 'in minutes, how long to wait for pulling image and starting container, does not apply once the container is running. Use action timeout_minute to timeout the overall Job run'
    required: false
    default: "10"
  backoffLimit:
    description: 'how many times to retry running the Job'
    required: false
    default: "1"
  requestsCPU:
    description: 'how much CPU resources request from system to run this job'
    required: true
    default: "1"
  requestsMEM:
    description: 'how much MEM resources request from system to run this job'
    required: true
    default: 1Gi
  limitCPU:
    description: 'how much CPU resources this job can consume'
    required: true
    default: "2"
  limitMEM:
    description: 'how much MEM resources this job can consume'
    required: true
    default: 2Gi
runs:
  using: "composite"
  steps:
    - name: "Check inputs"
      shell: bash
      run: |
        echo "     Check input:"
        echo "namespace: ${{ inputs.namespace }}"
        echo "name: ${{ inputs.name }}"
        echo "image: ${{ inputs.image }}"
        echo "command: ${{ inputs.command }}"
        echo "createNamespace: ${{ inputs.createNamespace }}"
        echo "deleteNamespace: ${{ inputs.cleanNamespace }}"

    - name: "Enable Problem Matcher"
      shell: bash
      run: |
        echo "::add-matcher::${{ github.action_path }}/.github/problem_matchers/eslint.json"
        echo "::add-matcher::${{ github.action_path }}/.github/problem_matchers/stylelint.json"

    - name: "Create Namespace"
      shell: bash
      run: |
        [[ "${{ inputs.createNamespace }}" != "true" ]] && exit 0
        echo "     Re-create namespace"
        kubectl delete namespace ${{ inputs.namespace }} || true
        kubectl create namespace ${{ inputs.namespace }}

    - name: "Create Job"
      shell: bash
      run: |
        cat <<EOF > job.yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: ${{ inputs.name }}
          namespace: ${{ inputs.namespace }}
        spec:
          backoffLimit: ${{ inputs.backoffLimit }}
          completions: 1
          parallelism: 1
          template:
            spec:
              # don't restart Pod, prefer Job backofflimit (easier to keep track off and wont end in an endless restart loop)
              restartPolicy: Never
              tolerations:
              - key: "ci-workloads-only"
                operator: "Exists"
              nodeSelector:
                reserved-for: ci-workloads
              volumes:
                - name: v1
                  emptyDir: {}
              containers:
                - name: extractor
                  image: bash:5.0.18
                  command: [ "/usr/local/bin/bash", "-c", "--" ]
                  args: [ "while true; do sleep 30; done;" ]
                  volumeMounts:
                    - mountPath: /job_outputs
                      name: v1                  
                - name: ${{ inputs.name }}
                  image: ${{ inputs.image }}
                  imagePullPolicy: Always
                  tty: true
                  resources:
                   limits:
                     cpu: "${{ inputs.limitCPU }}"
                     memory: ${{ inputs.limitMEM }}
                   requests:
                     cpu: "${{ inputs.requestsCPU }}"
                     memory: ${{ inputs.requestsMEM }}
                  command: ${{ inputs.command }}
                  volumeMounts:
                    - mountPath: /job_outputs
                      name: v1
                  env:
                      - name: "TERM"
                        value: "xterm-256color"
                      - name: "CI"
                        value: "true"
        EOF

        for e in $(printenv); do
          IFS='=' read -r -a key_val  <<< "$e"
          name=${key_val[0]}
          [[ "${name}" != _* ]] && continue
          name=${name:1}
          [[ "${name}" == "" ]] && continue
          # indent has to match the above heredoc
          cat <<EOF >> job.yaml
                      - name: "${name}"
                        value: "${key_val[1]}"
        EOF
        done

        echo "     Job to be created:"
        cat job.yaml
        echo "     Creating Job ${{ inputs.name }}"
        kubectl apply -f job.yaml

    - name: "Handle Job"
      shell: bash
      run: |
        # this crancky implementation is because '--pod-running-timeout' is not working...

        set -e
        set -x
        jobName=${{ inputs.name }}
        namespace=${{ inputs.namespace }}
        timeoutMinuteStartContainer=${{ inputs.timeoutMinuteStartContainer }}

        function yellAndExit1(){
          echo -e "\033[38;5;202mK8S Debug Information:\033[0m"
          echo -e ::group:: "[expand]"

          echo "     Namespace ${namespace} will not be deleted to allow debugging"
          echo "     Attempting to get as much info as possible before exiting 1"
          set -x
          kubectl -n ${namespace} get job/${jobName} -o yaml || true
          kubectl -n ${namespace} describe job/${jobName}  || true
          kubectl -n ${namespace} describe pod -l "job-name=${jobName}" || true
          echo ::endgroup::
          exit 1
        }

        # unique identifier for that Job, so we can query ressources for that specific Job without worrying about eventual previous run
        jobUid=$(kubectl -n ${namespace} get job/${jobName} -o jsonpath='{.spec.selector.matchLabels.controller-uid}')

        echo "     Keep trying to get logs until backoffLimit has been reached (or Job succeed)"
        while true; do

          echo "     Wait for the most recently created Pod to not be 'Pending' so logs can be fetched without errors"
          finaldate=$(date -d " ${timeoutMinuteStartContainer} minutes" +'%m/%d %H:%M')
          ready=false
          while [[ $ready != "true" ]]; do
            if [[ "$(date +'%m/%d %H:%M')" > "${finaldate}" ]]; then
              echo "     Err: Timeout waiting for pod to start"
              yellAndExit1
            fi
            echo "... waiting"
            sleep 10
            # check if the Job is finally active, or maybe already done
            jobPodPhases=$(kubectl -n ${namespace} get pod -l controller-uid=${jobUid} --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1:].status.phase}')
            # pods phase can be 'Pending' 'Running' 'Failed' 'Succeeded'
            [[ ${jobPodPhases} != 'Pending' ]] && ready=true
          done
          echo "     Job is either 'Running' 'Failed' 'Succeeded'"

          echo "     Attempt to fetch logs"
          echo "-----------------------------"
          echo ""
          kubectl -n ${namespace} logs --follow pod/"$(kubectl -n ${namespace} get pod -l controller-uid=${jobUid} --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1:].metadata.name}')" -c ${{ inputs.name }}|| true
          echo ""
          echo "-----------------------------"

          echo "     Job replica is done, checking Job status"
          # not elegant but the safest way to get the overall Job status as .failed and conditions start to get tricky
          # to look into as long as more than backofflimit is not 0
          # give 5s to Kubernetes to have time to update the job status
          
          # kubectl -n ${namespace} wait --for=condition=complete --timeout=60s job/${jobName} && complete=true || true
        
          terminate_status=`kubectl -n ci-kraken-26957 get pod -l controller-uid=${jobUid} \
                            --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1:].status.containerStatuses}' |\
                            jq -r '.[] | select( .name | contains("${{ inputs.name }}")) | .state.terminated.reason'`
        
          if [[ "${terminate_status}" == "Completed" ]]; then
            echo "     Job final state is 'complete', it ended with sucess"
            echo "     Extracting artifacts..."
            kubectl cp ${{ inputs.namespace }}/${{ inputs.name }}:/job_outputs ./ -c extractor
            exit 0
          else
            backoff_limit=$(kubectl -n ${namespace} get job/${jobName} -o jsonpath='{.spec.backoffLimit}')
            failed_pods=$(kubectl -n ${namespace} get job/${jobName} -o jsonpath='{.status.failed}')
            if [[ ${backoff_limit} -ge ${failed_pods} ]]; then
              echo "     Job replica failed, loop to wait for the next replica"
            else
              echo "     Job has reach its backoffLimit and its final state is not 'complete', it ended with failures"
              yellAndExit1
            fi
          fi
        done


    - name: "Clean Resources"
      shell: bash
      run: |
        [[ "${{ inputs.deleteNamespace }}" != "true" ]] && exit 0
        echo "     Delete namespace"
        kubectl delete namespace ${{ inputs.namespace }} || true
